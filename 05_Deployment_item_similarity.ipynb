{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2e6095-2729-49c3-950a-e4c046f1a522",
   "metadata": {},
   "source": [
    "# Deploiement d'une pipeline d'item similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2310fc7e-b99a-46e1-acab-f143407264a6",
   "metadata": {},
   "source": [
    "Ce notebook a pour but de créer un ensemble triton pour déployer une pipeline d'item similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93abcf37-7130-4d3c-a01c-972505d8d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install \"feast<0.31\" faiss-gpu\n",
    "#!pip install seedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb4c5466-dbd4-4f33-9f36-9ba9a6761545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py:29: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils as _distutils\n",
      "2024-09-04 09:00:45.338569: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-04 09:00:45.481575: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/dtypes/mappings/torch.py:43: UserWarning: PyTorch dtype mappings did not load successfully due to an error: No module named 'torch'\n",
      "  warn(f\"PyTorch dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/usr/local/lib/python3.10/dist-packages/nvtabular/loader/__init__.py:19: DeprecationWarning: The `nvtabular.loader` module has moved to a new repository, at https://github.com/NVIDIA-Merlin/dataloader .  Support for importing from `nvtabular.loader` is deprecated, and will be removed in a future version. Please update your imports to refer to `merlinloader`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import feast\n",
    "import seedir as sd\n",
    "from nvtabular import ColumnSchema, Schema\n",
    "from nvtabular import ColumnSelector, Workflow, Dataset\n",
    "from nvtabular.ops import Operator\n",
    "\n",
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "from merlin.systems.dag.ops.softmax_sampling import SoftmaxSampling\n",
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "from merlin.systems.dag.ops.unroll_features import UnrollFeatures\n",
    "from merlin.systems.triton.utils import send_triton_request\n",
    "from merlin.systems.dag.ops.workflow import TransformWorkflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad049f7-4419-4fd9-8ee9-1627394e2f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'feast' from '/usr/local/lib/python3.10/dist-packages/feast/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f8dff-92a5-4335-bf78-f318946842a9",
   "metadata": {},
   "source": [
    "## Enregistrement des embeddings dans le feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb176315-4549-466a-a7cc-2f165fdbdf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", \"/root/Data/Row/\")\n",
    "DATA_FOLDER = os.environ.get(\"DATA_FOLDER\", \"/root/Data/\")\n",
    "MODELS_FOLDER = os.environ.get(\"MODELS\", \"/root/Models/\")\n",
    "PROCESSED_FOLDER = os.environ.get(\"PROCESSED_FOLDER\", \"/root/Data/Processed/\")\n",
    "feature_repo_path = os.environ.get(\"FEAST_PATH\", \"/root/Data/feast_repo/feature_repo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c177117-788d-463a-88d4-70380a32d6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'cudf' from '/usr/local/lib/python3.10/dist-packages/cudf/__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from merlin.core.dispatch import get_lib\n",
    "df_lib = get_lib()\n",
    "df_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d5e186-8090-4340-ab08-61f1c87a943a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_id       int64\n",
       "embedding    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feast_embeddings = pd.read_csv('/root/Data/item_embeddings_for_similarity/feast_items_embeddings.csv')\n",
    "faiss_embeddings = pd.read_csv('/root/Data/item_embeddings_for_similarity/faiss_items_embeddings.csv')\n",
    "faiss_embeddings = faiss_embeddings[['item_id',\t'embedding']]\n",
    "faiss_embeddings.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3ce265d-ae5f-4658-b32f-f34f4ec625eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "faiss_embeddings[\"datetime\"] = datetime.now()\n",
    "faiss_embeddings[\"datetime\"] = faiss_embeddings[\"datetime\"].astype(\"datetime64[ns]\")\n",
    "faiss_embeddings[\"created\"] = datetime.now()\n",
    "faiss_embeddings[\"created\"] = faiss_embeddings[\"created\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e17dcf9-a974-4fb7-b76a-168536b0b864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id_e             int64\n",
      "embedding            object\n",
      "datetime     datetime64[ns]\n",
      "created      datetime64[ns]\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id_e</th>\n",
       "      <th>embedding</th>\n",
       "      <th>datetime</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108775015</td>\n",
       "      <td>[0.051989615, -0.008208182, 0.07192026, 0.0078...</td>\n",
       "      <td>2024-09-04 09:00:54.313023</td>\n",
       "      <td>2024-09-04 09:00:54.315691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108775044</td>\n",
       "      <td>[0.027522443, 0.009684976, -0.024995811, 0.030...</td>\n",
       "      <td>2024-09-04 09:00:54.313023</td>\n",
       "      <td>2024-09-04 09:00:54.315691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id_e                                          embedding  \\\n",
       "0  108775015  [0.051989615, -0.008208182, 0.07192026, 0.0078...   \n",
       "1  108775044  [0.027522443, 0.009684976, -0.024995811, 0.030...   \n",
       "\n",
       "                    datetime                    created  \n",
       "0 2024-09-04 09:00:54.313023 2024-09-04 09:00:54.315691  \n",
       "1 2024-09-04 09:00:54.313023 2024-09-04 09:00:54.315691  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss_embeddings.columns = ['item_id_e', 'embedding', 'datetime', 'created']\n",
    "faiss_embeddings[\"embedding\"] = faiss_embeddings[\"embedding\"].apply(lambda x: np.array(eval(x), dtype=np.float32))\n",
    "\n",
    "# Vérifiez les types de données\n",
    "print(faiss_embeddings.dtypes)\n",
    "faiss_embeddings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bcbb41d-2a47-49f3-a277-6397b347450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/root/Data/feast_repo/feature_repo/data/item_embeddings.parquet' \n",
    "faiss_embeddings.to_parquet(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b6be26c-d8ea-4db8-808d-78cefaec44b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id_e</th>\n",
       "      <th>embedding</th>\n",
       "      <th>datetime</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108775015</td>\n",
       "      <td>[0.051989615, -0.008208182, 0.07192026, 0.0078...</td>\n",
       "      <td>2024-09-04 09:00:54.313023</td>\n",
       "      <td>2024-09-04 09:00:54.315691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108775044</td>\n",
       "      <td>[0.027522443, 0.009684976, -0.024995811, 0.030...</td>\n",
       "      <td>2024-09-04 09:00:54.313023</td>\n",
       "      <td>2024-09-04 09:00:54.315691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id_e                                          embedding  \\\n",
       "0  108775015  [0.051989615, -0.008208182, 0.07192026, 0.0078...   \n",
       "1  108775044  [0.027522443, 0.009684976, -0.024995811, 0.030...   \n",
       "\n",
       "                    datetime                    created  \n",
       "0 2024-09-04 09:00:54.313023 2024-09-04 09:00:54.315691  \n",
       "1 2024-09-04 09:00:54.313023 2024-09-04 09:00:54.315691  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss_embeddings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f258b5ac-a49f-43eb-8e04-53cb0e5a4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(feature_repo_path, \"item_embeddings_4_similarity.py\"), \"w\")\n",
    "f.write(\n",
    "    \"\"\"\n",
    "from datetime import timedelta\n",
    "from feast import Entity, Field, FeatureView, ValueType\n",
    "from feast.types import Int32, Float32, Array\n",
    "from feast.infra.offline_stores.file_source import FileSource\n",
    "\n",
    "item_embeddings = FileSource(\n",
    "    path=\"{}\",\n",
    "    timestamp_field=\"datetime\",\n",
    "    created_timestamp_column=\"created\",\n",
    ")\n",
    "\n",
    "item = Entity(name=\"item_id_e\", value_type=ValueType.INT32,)\n",
    "\n",
    "item_embeddings_view = FeatureView(\n",
    "    name=\"item_embeddings\",\n",
    "    entities=[item],\n",
    "    ttl=timedelta(0),\n",
    "    schema=[\n",
    "        Field(name=\"embedding\", dtype=Array(Float32)),\n",
    "    ],\n",
    "    online=True,\n",
    "    source=item_embeddings,\n",
    "    tags=dict(),\n",
    ")\n",
    "\"\"\".format(\n",
    "        os.path.join(feature_repo_path, \"data/\", \"item_embeddings.parquet\")\n",
    "    )\n",
    ")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac62b171-2b6f-4166-b9f6-e595db50ccf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_repo/\n",
      "├─__init__.py\n",
      "├─__pycache__/\n",
      "│ ├─__init__.cpython-310.pyc\n",
      "│ ├─example_repo.cpython-310.pyc\n",
      "│ └─test_workflow.cpython-310.pyc\n",
      "├─cufile.log\n",
      "├─data/\n",
      "│ ├─item_embeddings.parquet\n",
      "│ ├─item_features.parquet\n",
      "│ ├─online_store.db\n",
      "│ ├─registry.db\n",
      "│ └─user_features.parquet\n",
      "├─feature_store.yaml\n",
      "├─item_embeddings_4_similarity.py\n",
      "├─item_features.py\n",
      "├─test_workflow.py\n",
      "└─user_features.py\n"
     ]
    }
   ],
   "source": [
    "import seedir as sd\n",
    "\n",
    "feature_repo_path = os.path.join(feature_repo_path)\n",
    "sd.seedir(\n",
    "    feature_repo_path,\n",
    "    style=\"lines\",\n",
    "    itemlimit=10,\n",
    "    depthlimit=3,\n",
    "    #exclude_folders=\".ipynb_checkpoints\",\n",
    "    sort=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7e2252e-3c48-4074-b682-663bc4cb77d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Data/feast_repo/feature_repo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created entity \u001b[1m\u001b[32muser_id\u001b[0m\n",
      "Created entity \u001b[1m\u001b[32mitem_id\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32muser_features\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32mitem_features\u001b[0m\n",
      "\n",
      "Created sqlite table \u001b[1m\u001b[32mfeast_repo_item_features\u001b[0m\n",
      "Created sqlite table \u001b[1m\u001b[32mfeast_repo_user_features\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd $feature_repo_path\n",
    "!find . -name \".ipynb_checkpoints\" -exec rm -r {} +\n",
    "!feast apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c3a565e-9be2-4091-9a00-38e5a799e22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m3\u001b[0m feature views from \u001b[1m\u001b[32m1995-01-01 01:01:01+00:00\u001b[0m to \u001b[1m\u001b[32m2025-01-01 01:01:01+00:00\u001b[0m into the \u001b[1m\u001b[32msqlite\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mitem_embeddings\u001b[0m:\n",
      "100%|██████████████████████████████████████████████████████| 105100/105100 [01:46<00:00, 990.27it/s]\n",
      "\u001b[1m\u001b[32muser_features\u001b[0m:\n",
      "100%|█████████████████████████████████████████████████████| 442707/442707 [04:40<00:00, 1578.42it/s]\n",
      "\u001b[1m\u001b[32mitem_features\u001b[0m:\n",
      "100%|███████████████████████████████████████████████████████| 23417/23417 [00:12<00:00, 1946.19it/s]\n"
     ]
    }
   ],
   "source": [
    "!feast materialize 1995-01-01T01:01:01 2025-01-01T01:01:01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fad0745-f23c-4dc3-89b3-9d5def8404c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store = feast.FeatureStore(feature_repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d57c0ce-889f-4a3f-bd97-8860a3a51d44",
   "metadata": {},
   "source": [
    "## Creation de l'index Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71e224f2-3213-48ab-b939-33b9f853003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.systems.dag.ops.faiss import QueryFaiss, setup_faiss\n",
    "\n",
    "faiss_index_path = os.path.join(DATA_FOLDER, 'faiss_index', \"index_item_similarity.faiss\")\n",
    "faiss_embeddings = faiss_embeddings[['item_id_e', 'embedding']]\n",
    "faiss_embeddings.columns = ['item_id', 'embedding']\n",
    "setup_faiss(faiss_embeddings, faiss_index_path, embedding_column=\"embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf4dbe7-94bb-4d85-b3af-0c402c0bb32b",
   "metadata": {},
   "source": [
    "## Creation de la pipeline d'item similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c056fb8-0c32-457e-ba79-b80fa5bc5107",
   "metadata": {},
   "source": [
    "Operateur Faiss personnalisé pour que ça marche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fab3fc64-8176-45a4-8462-041d6f97f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38aed5a1-2a5a-4b39-980f-c23366d9ca84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef setup_faiss(\\n    item_vector: DataFrameLike,\\n    output_path: str,\\n    metric=faiss.METRIC_INNER_PRODUCT,\\n    item_id_column=\"item_id\",\\n    embedding_column=\"embedding\",\\n):\\n    \"\"\"\\n    Utiltiy function that will create a Faiss index from a set of embedding vectors\\n\\n    Parameters\\n    ----------\\n    item_vector : Numpy.ndarray\\n        This is a matrix representing all the nodes embeddings, represented as a numpy ndarray.\\n    output_path : string\\n        target output path\\n    \"\"\"\\n    ids = item_vector[item_id_column].to_numpy().astype(np.int64)\\n    item_vectors = np.ascontiguousarray(\\n        np.stack(item_vector[embedding_column].to_numpy()).astype(np.float32)\\n    )\\n\\n    index = faiss.index_factory(item_vectors.shape[1], \"IVF32,Flat\", metric)\\n    index.nprobe = 8\\n\\n    index.train(item_vectors)\\n    index.add_with_ids(item_vectors, ids)\\n    faiss.write_index(index, str(output_path))\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Copyright (c) 2022, NVIDIA CORPORATION.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "import os\n",
    "from pathlib import Path\n",
    "from shutil import copy2\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import cupy\n",
    "\n",
    "from merlin.core.dispatch import HAS_GPU\n",
    "from merlin.core.protocols import DataFrameLike, Transformable\n",
    "from merlin.dag import BaseOperator, ColumnSelector\n",
    "from merlin.schema import ColumnSchema, Schema\n",
    "\n",
    "\n",
    "class QueryFaissCustomed(BaseOperator):\n",
    "    \"\"\"\n",
    "    This operator creates an interface between a FAISS[1] Approximate Nearest Neighbors (ANN)\n",
    "    Index and Triton Infrence Server. The operator allows users to perform different supported\n",
    "    types[2] of Nearest Neighbor search to your ensemble. For input query vector, we do an ANN\n",
    "    search query to find the ids of top-k nearby nodes in the index.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] https://github.com/facebookresearch/faiss)\n",
    "    [2] https://github.com/facebookresearch/faiss/wiki/Faiss-indexes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, index_path, topk=10):\n",
    "        \"\"\"\n",
    "        Creates a QueryFaiss Pipelineable Inference Operator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index_path : str\n",
    "            A path to an already setup index\n",
    "        topk : int, optional\n",
    "            The number of results we should receive from query to Faiss as output, by default 10\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.index_path = str(index_path)\n",
    "        self.topk = topk\n",
    "        self._index = None\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def load_artifacts(self, artifact_path: str) -> None:\n",
    "        if artifact_path:\n",
    "            filename = Path(self.index_path).name\n",
    "            path_artifact = Path(artifact_path)\n",
    "            if path_artifact.is_file():\n",
    "                path_artifact = path_artifact.parent\n",
    "            full_index_path = str(path_artifact / filename)\n",
    "        else:\n",
    "            full_index_path = self.index_path\n",
    "        index = faiss.read_index(full_index_path)\n",
    "\n",
    "        if HAS_GPU:\n",
    "            res = faiss.StandardGpuResources()\n",
    "            index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "        self._index = index\n",
    "\n",
    "    def save_artifacts(self, artifact_path: str) -> None:\n",
    "        index_filename = os.path.basename(os.path.realpath(self.index_path))\n",
    "        new_index_path = Path(artifact_path) / index_filename\n",
    "        copy2(self.index_path, new_index_path)\n",
    "\n",
    "    def __getstate__(self) -> dict:\n",
    "        \"\"\"Return state of instance when pickled.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Returns object state excluding index attribute.\n",
    "        \"\"\"\n",
    "        return {k: v for k, v in self.__dict__.items() if k != \"_index\"}\n",
    "\n",
    "    def transform(\n",
    "        self, col_selector: ColumnSelector, transformable: Transformable\n",
    "    ) -> Transformable:\n",
    "        \"\"\"\n",
    "        Transform input dataframe to output dataframe using function logic.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : TensorTable\n",
    "            Input tensor dictionary, data that will be manipulated\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        TensorTable\n",
    "            Transformed tensor dictionary\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "        if isinstance(transformable, dict):\n",
    "            user_vector = list(transformable.values())[0]\n",
    "        elif hasattr(transformable, 'to_dict'):\n",
    "            dict_values = list(transformable.to_dict().values())\n",
    "            if dict_values and isinstance(dict_values[0], dict):\n",
    "                user_vector = list(dict_values[0].values())[0]\n",
    "            else:\n",
    "                user_vector = dict_values[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected input type: {type(transformable)}\")\n",
    "    \n",
    "        # Convertir en numpy array si ce n'est pas déjà le cas\n",
    "        if not isinstance(user_vector, (np.ndarray, cupy.ndarray)):\n",
    "            user_vector = np.array(user_vector)\n",
    "        \n",
    "        self.logger.error(f\"User vector shape before reshape: {user_vector.shape}\")\n",
    "        self.logger.error(f\"User vector ndim before reshape: {user_vector.ndim}\")\n",
    "    \n",
    "        # S'assurer que user_vector est un tableau 2D\n",
    "        \n",
    "        user_vector = user_vector.reshape(1, -1)\n",
    "    \n",
    "        self.logger.error(f\"User vector shape after reshape: {user_vector.shape}\")\n",
    "        self.logger.error(f\"User vector ndim after reshape: {user_vector.ndim}\")\n",
    "        self.logger.error(f\"Index dimension: {self._index.d}\")\n",
    "    \n",
    "        # Vérifier que la dimension correspond à celle attendue par l'index\n",
    "        if user_vector.shape[1] != self._index.d:\n",
    "            raise ValueError(f\"User vector dimension ({user_vector.shape[1]}) does not match index dimension ({self._index.d}), here vector {user_vector}\")\n",
    "    \n",
    "        \n",
    "        _, indices = self._index.search(user_vector, self.topk)\n",
    "\n",
    "        candidate_ids = np.array(indices).astype(np.int32).flatten()\n",
    "\n",
    "        return type(transformable)({\"candidate_ids\": candidate_ids})\n",
    "\n",
    "    def compute_input_schema(\n",
    "        self,\n",
    "        root_schema: Schema,\n",
    "        parents_schema: Schema,\n",
    "        deps_schema: Schema,\n",
    "        selector: ColumnSelector,\n",
    "    ) -> Schema:\n",
    "        \"\"\"\n",
    "        Compute the input schema of this node given the root, parents and dependencies schemas of\n",
    "        all ancestor nodes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        root_schema : Schema\n",
    "            The schema representing the input columns to the graph\n",
    "        parents_schema : Schema\n",
    "            A schema representing all the output columns of the ancestors of this node.\n",
    "        deps_schema : Schema\n",
    "            A schema representing the dependencies of this node.\n",
    "        selector : ColumnSelector\n",
    "            A column selector representing a target subset of columns necessary for this node's\n",
    "            operator\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Schema\n",
    "            A schema that has the correct representation of all the incoming columns necessary for\n",
    "            this node's operator to complete its transform.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            Cannot receive more than one input for this node\n",
    "        \"\"\"\n",
    "        input_schema = super().compute_input_schema(\n",
    "            root_schema, parents_schema, deps_schema, selector\n",
    "        )\n",
    "        return input_schema\n",
    "\n",
    "    def compute_output_schema(\n",
    "        self, input_schema: Schema, col_selector: ColumnSelector, prev_output_schema: Schema = None\n",
    "    ) -> Schema:\n",
    "        \"\"\"\n",
    "        Compute the input schema of this node given the root, parents and dependencies schemas of\n",
    "        all ancestor nodes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_schema : Schema\n",
    "            The schema representing the input columns to the graph\n",
    "        col_selector : ColumnSelector\n",
    "            A column selector representing a target subset of columns necessary for this node's\n",
    "            operator\n",
    "        prev_output_schema : Schema\n",
    "            A schema representing the output of the previous node.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Schema\n",
    "            A schema object representing all outputs of this node.\n",
    "        \"\"\"\n",
    "        return Schema(\n",
    "            [\n",
    "                ColumnSchema(\"candidate_ids\", dtype=np.int32, dims=(None, self.topk)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def validate_schemas(\n",
    "        self, parents_schema, deps_schema, input_schema, output_schema, strict_dtypes=False\n",
    "    ):\n",
    "        if len(input_schema.column_schemas) > 1:\n",
    "            raise ValueError(\n",
    "                \"More than one input has been detected for this node,\"\n",
    "                / f\"inputs received: {input_schema.column_names}\"\n",
    "            )\n",
    "\n",
    "'''\n",
    "def setup_faiss(\n",
    "    item_vector: DataFrameLike,\n",
    "    output_path: str,\n",
    "    metric=faiss.METRIC_INNER_PRODUCT,\n",
    "    item_id_column=\"item_id\",\n",
    "    embedding_column=\"embedding\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Utiltiy function that will create a Faiss index from a set of embedding vectors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    item_vector : Numpy.ndarray\n",
    "        This is a matrix representing all the nodes embeddings, represented as a numpy ndarray.\n",
    "    output_path : string\n",
    "        target output path\n",
    "    \"\"\"\n",
    "    ids = item_vector[item_id_column].to_numpy().astype(np.int64)\n",
    "    item_vectors = np.ascontiguousarray(\n",
    "        np.stack(item_vector[embedding_column].to_numpy()).astype(np.float32)\n",
    "    )\n",
    "\n",
    "    index = faiss.index_factory(item_vectors.shape[1], \"IVF32,Flat\", metric)\n",
    "    index.nprobe = 8\n",
    "\n",
    "    index.train(item_vectors)\n",
    "    index.add_with_ids(item_vectors, ids)\n",
    "    faiss.write_index(index, str(output_path))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a55e1af-7bdf-4ec2-871e-5dca6c281294",
   "metadata": {},
   "source": [
    "Création de la pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b33f385-d2e8-4a50-89ac-e855db540f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 09:09:16,513 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): usage.feast.dev:443\n",
      "2024-09-04 09:09:16,520 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): usage.feast.dev:443\n",
      "2024-09-04 09:09:16,524 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): usage.feast.dev:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m1\u001b[0m feature views to \u001b[1m\u001b[32m2024-09-04 09:09:16+00:00\u001b[0m into the \u001b[1m\u001b[32msqlite\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mitem_embeddings\u001b[0m from \u001b[1m\u001b[32m2025-01-01 01:01:01+00:00\u001b[0m to \u001b[1m\u001b[32m2024-09-04 09:09:16+00:00\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 09:09:16,799 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): usage.feast.dev:443\n",
      "0it [00:00, ?it/s]\n",
      "2024-09-04 09:09:19,205 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): usage.feast.dev:443\n"
     ]
    }
   ],
   "source": [
    "from merlin.core.dispatch import make_df\n",
    "from merlin.systems.dag.ops.feast import QueryFeast\n",
    "faiss_index_path = os.path.join(DATA_FOLDER, 'faiss_index', \"index_item_similarity.faiss\")\n",
    "\n",
    "#Test input\n",
    "request = make_df({\"item_id_e\": [688463003]})\n",
    "request[\"item_id_e\"] = request[\"item_id_e\"].astype(np.int32)\n",
    "test_dataset = Dataset(request)\n",
    "\n",
    "# Embedding retrieval\n",
    "item_embedding_values = [\"item_id_e\"] >> QueryFeast.from_feature_view(\n",
    "    store=feature_store,\n",
    "    view=\"item_embeddings\",\n",
    "    column=\"item_id_e\",\n",
    "    include_id=False,\n",
    ")\n",
    "\n",
    "# Similarity comparison\n",
    "topk_items_similarity = int(\n",
    "    os.environ.get(\"topk_items_similarity\", \"8\")\n",
    ")\n",
    "\n",
    "similar_items = item_embedding_values >> QueryFaissCustomed(faiss_index_path, topk=topk_items_similarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0fbb0ac-a48a-4379-b61a-10cf1223b37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 08:49:37,561 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): usage.feast.dev:443\n",
      "2024-09-04 08:56:45,619 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): usage.feast.dev:443\n",
      "2024-09-04 08:56:46,265 - QueryFaissCustomed - ERROR - User vector shape before reshape: (512,)\n",
      "2024-09-04 08:56:46,266 - QueryFaissCustomed - ERROR - User vector ndim before reshape: 1\n",
      "2024-09-04 08:56:46,266 - QueryFaissCustomed - ERROR - User vector shape after reshape: (1, 512)\n",
      "2024-09-04 08:56:46,267 - QueryFaissCustomed - ERROR - User vector ndim after reshape: 2\n",
      "2024-09-04 08:56:46,267 - QueryFaissCustomed - ERROR - Index dimension: 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   candidate_ids\n",
      "0      688463001\n",
      "1      688463003\n",
      "2      812371001\n",
      "3      736870001\n",
      "4      736870005\n",
      "5      108775015\n",
      "6      863937010\n",
      "7      626366003\n"
     ]
    }
   ],
   "source": [
    "# Exécuter la pipeline sur les données de test\n",
    "similarity_workflow = Workflow(similar_items)\n",
    "output = similarity_workflow.transform(test_dataset)\n",
    "\n",
    "\n",
    "print(output.to_ddf().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993585f-5f1c-47ed-a637-94153704c964",
   "metadata": {},
   "source": [
    "Préparation de tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f1a6d5d-ac22-4931-96bf-01e3bdb7725d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.dlpack.dlpack.from_dlpack(dlcapsule)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prevent TF to claim all GPU memory\n",
    "from merlin.dataloader.tf_utils import configure_tensorflow\n",
    "configure_tensorflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b32f8e-2a84-4711-baaa-af824f1b7d1c",
   "metadata": {},
   "source": [
    "## Sauvegarde de l'ensemble Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b09724c-0e9c-4c9c-bccb-862de930a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"/root/Triton_models\"):\n",
    "    os.makedirs(os.path.join('/root/Triton_models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "233f23bb-83ba-448d-a616-b9c7605e3d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   item_id_e\n",
      "0  688463003\n",
      "[{'name': 'item_id_e', 'tags': set(), 'properties': {}, 'dtype': DType(name='int32', element_type=<ElementType.Int: 'int'>, element_size=32, element_unit=None, signed=True, shape=Shape(dims=None)), 'is_list': False, 'is_ragged': False}]\n"
     ]
    }
   ],
   "source": [
    "from merlin.core.dispatch import make_df\n",
    "\n",
    "# create a request to be sent to TIS\n",
    "request = make_df({\"item_id_e\": [688463003]})\n",
    "request[\"item_id_e\"] = request[\"item_id_e\"].astype(np.int32)\n",
    "print(request)\n",
    "\n",
    "request_schema = Schema(\n",
    "    [\n",
    "        ColumnSchema(\"item_id_e\", dtype=np.int32),\n",
    "    ]\n",
    ")\n",
    "print(request_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5fa00b-774a-43a0-ae7d-10abe9737565",
   "metadata": {},
   "source": [
    "Création de l'ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67faa682-a4e7-4077-b525-e993fac39d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 09:09:19,227 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): usage.feast.dev:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['candidate_ids']\n",
      "CPU times: user 7min 29s, sys: 1.81 s, total: 7min 31s\n",
      "Wall time: 6min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define the path where all the models and config files exported to\n",
    "export_path = os.path.join('/root/Triton_models_test_operateur_perso')\n",
    "\n",
    "ensemble = Ensemble(similar_items, request_schema)\n",
    "ens_config, node_configs = ensemble.export(export_path, name='item_similarity')\n",
    "\n",
    "# return the output column name\n",
    "outputs = ensemble.graph.output_schema.column_names\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0233a7b-65b2-478d-b404-f446edaff759",
   "metadata": {},
   "source": [
    "Enlève les fichiers qui pourraient poser problème à Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a1c0083-d134-49b1-a2d6-c3786d19fe6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton_models_test_operateur_perso/\n",
      "├─0_transformworkflowtriton/\n",
      "│ ├─1/\n",
      "│ │ ├─__pycache__/\n",
      "│ │ │ └─model.cpython-310.pyc\n",
      "│ │ ├─model.py\n",
      "│ │ └─workflow/\n",
      "│ │   ├─categories/\n",
      "│ │   │ ├─unique.2nd_last_product_code.parquet\n",
      "│ │   │ ├─unique.2nd_last_product_type.parquet\n",
      "│ │   │ ├─unique.2nd_popular_department_no.parquet\n",
      "│ │   │ ├─unique.2nd_popular_product_type.parquet\n",
      "│ │   │ ├─unique.2nd_popular_section_no.parquet\n",
      "│ │   │ ├─unique.Active.parquet\n",
      "│ │   │ ├─unique.FN.parquet\n",
      "│ │   │ ├─unique.club_member_status.parquet\n",
      "│ │   │ ├─unique.fashion_news_frequency.parquet\n",
      "│ │   │ └─unique.last_product_code.parquet\n",
      "│ │   ├─metadata.json\n",
      "│ │   └─workflow.pkl\n",
      "│ └─config.pbtxt\n",
      "├─1_predicttensorflowtriton/\n",
      "│ ├─1/\n",
      "│ │ └─model.savedmodel/\n",
      "│ │   ├─.merlin/\n",
      "│ │   │ ├─input_schema.json\n",
      "│ │   │ └─output_schema.json\n",
      "│ │   ├─assets/\n",
      "│ │   ├─fingerprint.pb\n",
      "│ │   ├─keras_metadata.pb\n",
      "│ │   ├─saved_model.pb\n",
      "│ │   └─variables/\n",
      "│ │     ├─variables.data-00000-of-00001\n",
      "│ │     └─variables.index\n",
      "│ └─config.pbtxt\n",
      "├─2_transformworkflowtriton/\n",
      "│ ├─1/\n",
      "│ │ ├─__pycache__/\n",
      "│ │ │ └─model.cpython-310.pyc\n",
      "│ │ ├─model.py\n",
      "│ │ └─workflow/\n",
      "│ │   ├─categories/\n",
      "│ │   │ ├─unique.colour_group_code.parquet\n",
      "│ │   │ ├─unique.department_no.parquet\n",
      "│ │   │ ├─unique.detail_desc.parquet\n",
      "│ │   │ ├─unique.garment_group_no.parquet\n",
      "│ │   │ ├─unique.graphical_appearance_no.parquet\n",
      "│ │   │ ├─unique.index_code.parquet\n",
      "│ │   │ ├─unique.index_group_no.parquet\n",
      "│ │   │ ├─unique.perceived_colour_master_id.parquet\n",
      "│ │   │ ├─unique.perceived_colour_value_id.parquet\n",
      "│ │   │ └─unique.prod_name.parquet\n",
      "│ │   ├─metadata.json\n",
      "│ │   └─workflow.pkl\n",
      "│ └─config.pbtxt\n",
      "├─3_predicttensorflowtriton/\n",
      "│ ├─1/\n",
      "│ │ └─model.savedmodel/\n",
      "│ │   ├─.merlin/\n",
      "│ │   │ ├─input_schema.json\n",
      "│ │   │ └─output_schema.json\n",
      "│ │   ├─assets/\n",
      "│ │   ├─fingerprint.pb\n",
      "│ │   ├─keras_metadata.pb\n",
      "│ │   ├─saved_model.pb\n",
      "│ │   └─variables/\n",
      "│ │     ├─variables.data-00000-of-00001\n",
      "│ │     └─variables.index\n",
      "│ └─config.pbtxt\n",
      "├─executor_model/\n",
      "│ ├─1/\n",
      "│ │ ├─__pycache__/\n",
      "│ │ │ └─model.cpython-310.pyc\n",
      "│ │ ├─ensemble/\n",
      "│ │ │ ├─ensemble.pkl\n",
      "│ │ │ ├─index.faiss\n",
      "│ │ │ └─metadata.json\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "└─item_similarity/\n",
      "  ├─1/\n",
      "  │ ├─ensemble/\n",
      "  │ │ ├─ensemble.pkl\n",
      "  │ │ ├─index_item_similarity.faiss\n",
      "  │ │ └─metadata.json\n",
      "  │ └─model.py\n",
      "  └─config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "export_path = os.path.join('/root/Triton_models_test_operateur_perso')\n",
    "\n",
    "def remove_checkpoints(dir_path):\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for dir_name in dirs:\n",
    "            if dir_name == '.ipynb_checkpoints':\n",
    "                dir_to_remove = os.path.join(root, dir_name)\n",
    "                print(f\"Removing: {dir_to_remove}\")\n",
    "                shutil.rmtree(dir_to_remove)\n",
    "\n",
    "remove_checkpoints(export_path)\n",
    "\n",
    "sd.seedir(export_path, style='lines', itemlimit=10, depthlimit=5, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c480b-bff9-4264-acfc-dbc42654fd5a",
   "metadata": {},
   "source": [
    "## Démarrage du Triton Server\n",
    "Executer dans un terminal : \n",
    "\n",
    "tritonserver --model-repository=/root/Triton_models/ --backend-config=tensorflow,version=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3246ed93-d1f9-433b-ba47-fd39eae38c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.3 ms, sys: 7 µs, total: 24.3 ms\n",
      "Wall time: 69.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'candidate_ids': array([736870001, 736870005, 812371001, 688463001, 688463003, 626366003,\n",
       "        863937010, 108775015], dtype=int32)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from merlin.core.dispatch import make_df\n",
    "\n",
    "# create a request to be sent to TIS\n",
    "request = make_df({\"item_id_e\": [736870005]})\n",
    "request[\"item_id_e\"] = request[\"item_id_e\"].astype(np.int32)\n",
    "\n",
    "request_schema = Schema(\n",
    "    [\n",
    "        ColumnSchema(\"item_id_e\", dtype=np.int32),\n",
    "    ]\n",
    ")\n",
    "outputs = ['candidate_ids']\n",
    "response = send_triton_request(request_schema, request, outputs, triton_model='item_similarity')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef32925-7782-4127-a49f-48064248303f",
   "metadata": {},
   "source": [
    "Test de la première pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d93dddc8-f6ee-462a-98d6-59f641d6dfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.7 ms, sys: 6.9 ms, total: 20.6 ms\n",
      "Wall time: 1.62 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ordered_ids': array([[783751003, 887542001, 815542002, 817086002, 812432001, 697498001,\n",
       "         794054001, 880738003, 882066001, 888507002, 690617002, 697058001]],\n",
       "       dtype=int32),\n",
       " 'ordered_scores': array([[0.99187917, 0.5780467 , 0.99725145, 0.7040904 , 0.9999831 ,\n",
       "         0.98252654, 0.99974936, 0.4671511 , 0.66586274, 0.96997446,\n",
       "         0.99998343, 0.9998634 ]], dtype=float32)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# create a request to be sent to TIS\n",
    "\n",
    "request = make_df({\"user_id\": [11]})\n",
    "request[\"user_id\"] = request[\"user_id\"].astype(np.int32)\n",
    "\n",
    "outputs = ['ordered_ids', 'ordered_scores']\n",
    "\n",
    "request_schema = Schema(\n",
    "    [\n",
    "        ColumnSchema(\"user_id\", dtype=np.int32),\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = send_triton_request(request_schema, request, outputs, triton_model='executor_model')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94351f98-4876-4f5d-b19d-e650c3ec53e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
